{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 864
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 183373,
     "status": "ok",
     "timestamp": 1543544931836,
     "user": {
      "displayName": "Shared 98999",
      "photoUrl": "",
      "userId": "02966646165121999534"
     },
     "user_tz": 480
    },
    "id": "dJHdiFeXKBan",
    "outputId": "c1f36440-d15b-42fa-998b-da820615155e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3523 samples, validate on 392 samples\n",
      "Epoch 1/20\n",
      " - 60s - loss: 1.0906 - acc: 0.6267 - val_loss: 0.6054 - val_acc: 0.7347\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.73469, saving model to data/weights.h5\n",
      "Epoch 2/20\n",
      " - 54s - loss: 0.3200 - acc: 0.8788 - val_loss: 0.2306 - val_acc: 0.9464\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.73469 to 0.94643, saving model to data/weights.h5\n",
      "Epoch 3/20\n",
      " - 54s - loss: 0.0432 - acc: 0.9830 - val_loss: 0.2904 - val_acc: 0.9388\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.94643\n",
      "Epoch 4/20\n",
      " - 55s - loss: 0.0139 - acc: 0.9952 - val_loss: 0.2862 - val_acc: 0.9566\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.94643 to 0.95663, saving model to data/weights.h5\n",
      "Epoch 5/20\n",
      " - 53s - loss: 0.0071 - acc: 0.9972 - val_loss: 0.2533 - val_acc: 0.9592\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.95663 to 0.95918, saving model to data/weights.h5\n",
      "Epoch 6/20\n",
      " - 54s - loss: 0.0044 - acc: 0.9983 - val_loss: 0.2917 - val_acc: 0.9541\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.95918\n",
      "Epoch 7/20\n",
      " - 55s - loss: 0.0024 - acc: 0.9994 - val_loss: 0.3460 - val_acc: 0.9566\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.95918\n",
      "Epoch 8/20\n",
      " - 53s - loss: 0.0019 - acc: 0.9997 - val_loss: 0.3697 - val_acc: 0.9439\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.95918\n",
      "Epoch 9/20\n",
      " - 53s - loss: 0.0012 - acc: 0.9997 - val_loss: 0.3494 - val_acc: 0.9592\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.95918\n",
      "Epoch 10/20\n",
      " - 54s - loss: 5.8298e-04 - acc: 0.9997 - val_loss: 0.3159 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.95918 to 0.96173, saving model to data/weights.h5\n",
      "Epoch 11/20\n",
      " - 54s - loss: 3.1990e-04 - acc: 1.0000 - val_loss: 0.2897 - val_acc: 0.9668\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.96173 to 0.96684, saving model to data/weights.h5\n",
      "Epoch 12/20\n",
      " - 53s - loss: 2.5498e-04 - acc: 1.0000 - val_loss: 0.2810 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.96684\n",
      "Epoch 13/20\n",
      " - 53s - loss: 9.7293e-05 - acc: 1.0000 - val_loss: 0.2793 - val_acc: 0.9668\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.96684\n",
      "Epoch 14/20\n",
      " - 52s - loss: 1.9381e-04 - acc: 1.0000 - val_loss: 0.3015 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.96684\n",
      "Epoch 15/20\n",
      " - 53s - loss: 1.9575e-04 - acc: 1.0000 - val_loss: 0.2988 - val_acc: 0.9643\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.96684\n",
      "Epoch 16/20\n",
      " - 54s - loss: 1.3442e-04 - acc: 1.0000 - val_loss: 0.3040 - val_acc: 0.9566\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.96684\n",
      "Epoch 17/20\n",
      " - 53s - loss: 9.8336e-05 - acc: 1.0000 - val_loss: 0.2987 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.96684\n",
      "Epoch 18/20\n",
      " - 53s - loss: 1.4919e-04 - acc: 1.0000 - val_loss: 0.3536 - val_acc: 0.9643\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.96684\n",
      "Epoch 19/20\n",
      " - 3033s - loss: 4.1673e-05 - acc: 1.0000 - val_loss: 0.3480 - val_acc: 0.9643\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.96684\n",
      "Epoch 20/20\n",
      " - 28s - loss: 1.4213e-04 - acc: 1.0000 - val_loss: 0.3474 - val_acc: 0.9643\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.96684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16c91d5d6d8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "y_train = train.label.values\n",
    "y_test = test.label.values\n",
    "\n",
    "tk = text.Tokenizer(num_words=200000)\n",
    "train.link = train.link_name.astype(str)\n",
    "test.link = test.link_name.astype(str)\n",
    "train.text = train.textdata.astype(str)\n",
    "test.text = test.textdata.astype(str)\n",
    "\n",
    "max_len = 80\n",
    "\n",
    "tk.fit_on_texts(list(train.link.values) + list(train.text.values) + list(test.link.values) + list(\n",
    "    test.text.values))\n",
    "x_train_title = tk.texts_to_sequences(train.link.values)\n",
    "x_train_title = sequence.pad_sequences(x_train_title, maxlen=max_len)\n",
    "\n",
    "x_train_textdata_01 = tk.texts_to_sequences(train.text.values)\n",
    "x_train_textdata_01 = sequence.pad_sequences(x_train_textdata_01, maxlen=max_len)\n",
    "\n",
    "x_test_title_01 = tk.texts_to_sequences(test.link.values)\n",
    "x_test_title_01 = sequence.pad_sequences(x_test_title_01, maxlen=max_len)\n",
    "\n",
    "x_test_textdata_02 = tk.texts_to_sequences(test.text.values)\n",
    "x_test_textdata_02 = sequence.pad_sequences(x_test_textdata_02, maxlen=max_len)\n",
    "\n",
    "word_index = tk.word_index\n",
    "ytrain_enc = np_utils.to_categorical(y_train)\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Embedding(len(word_index), 300, input_length=80, dropout=0.2),)\n",
    "classifier.add(LSTM(300, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "classifier.add(Dense(200))\n",
    "classifier.add(PReLU())\n",
    "classifier.add(SpatialDropout1D(0.2))\n",
    "\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "classifier.add(Dense(200))\n",
    "classifier.add(PReLU())\n",
    "classifier.add(SpatialDropout1D(0.2))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(2))\n",
    "classifier.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc' ])\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('data/weights.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "\n",
    "classifier.fit(x_train_title, y=ytrain_enc,\n",
    "                 batch_size=128, epochs=20, verbose=2, validation_split=0.1,\n",
    "                 shuffle=True, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1HsRmhBKJL7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "05_dexters_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
