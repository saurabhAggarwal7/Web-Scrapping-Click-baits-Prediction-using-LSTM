{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2763
    },
    "colab_type": "code",
    "id": "Uv_nQlPcoaNv",
    "outputId": "b926a81e-56b0-4629-9316-135d224ccfae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2137/2137 [02:09<00:00, 16.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3019/3019 [02:01<00:00, 24.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup                          #scrape information from web pages\n",
    "from goose3 import Goose                               #extract information from news articles\n",
    "from collections import Counter                        \n",
    "import string\n",
    "from joblib import Parallel, delayed\n",
    "import sys\n",
    "from tqdm import tqdm                                    \n",
    "\n",
    "stop_domains = ['buzzfeed', 'clickhole', 'cnn', 'wikinews', 'upworthy', 'nytimes']\n",
    "\n",
    "\n",
    "def features(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        g = Goose()\n",
    "        try:\n",
    "            goose_article = g.extract(raw_html=html)\n",
    "        except TypeError:\n",
    "            goose_article = None\n",
    "        except IndexError:\n",
    "            goose_article = None\n",
    "\n",
    "        size = sys.getsizeof(html)\n",
    "        html_len = len(html)\n",
    "        count_links = len(soup.find_all('a'))\n",
    "        count_buttons = len(soup.find_all('button'))\n",
    "        count_inputs = len(soup.find_all('input'))\n",
    "        count_ul = len(soup.find_all('ul'))\n",
    "        count_ol = len(soup.find_all('ol'))\n",
    "        count_lists = count_ol + count_ul\n",
    "        count_h1 = len(soup.find_all('h1'))\n",
    "        count_h2 = len(soup.find_all('h2'))\n",
    "        if count_h1 > 0:\n",
    "            h1_len = 0\n",
    "            h1_text = ''\n",
    "            for x in soup.find_all('h1'):\n",
    "                text = x.get_text().strip()\n",
    "                h1_text += text + ' '\n",
    "                h1_len += len(text)\n",
    "            total_h1_len = h1_len\n",
    "            avg_h1_len = h1_len * 1. / count_h1\n",
    "        else:\n",
    "            total_h1_len = 0\n",
    "            avg_h1_len = 0\n",
    "            h1_text = ''\n",
    "\n",
    "        if count_h2 > 0:\n",
    "            h2_len = 0\n",
    "            h2_text = ''\n",
    "            for x in soup.find_all('h2'):\n",
    "                text = x.get_text().strip()\n",
    "                h2_len += len(text)\n",
    "                h2_text += text + ' '\n",
    "            total_h2_len = h2_len\n",
    "            avg_h2_len = h2_len * 1. / count_h2\n",
    "        else:\n",
    "            total_h2_len = 0\n",
    "            avg_h2_len = 0\n",
    "            h2_text = ''\n",
    "        if goose_article is not None:\n",
    "            parser_data = goose_article.meta_description + ' ' + h1_text + ' ' + h2_text\n",
    "            parser_data = \"\".join(l for l in parser_data if l not in string.punctuation)\n",
    "            parser_data = parser_data.strip().lower().split()\n",
    "            parser_data = [word for word in parser_data if word.lower() not in stop_domains]\n",
    "            parser_data = ' '.join(parser_data)\n",
    "        else:\n",
    "            parser_data = h1_text + ' ' + h2_text\n",
    "            parser_data = \"\".join(l for l in parser_data if l not in string.punctuation)\n",
    "            parser_data = parser_data.strip().lower().split()\n",
    "            parser_data = [word for word in parser_data if word.lower() not in stop_domains]\n",
    "            parser_data = ' '.join(parser_data)\n",
    "\n",
    "        count_images = len(soup.find_all('img'))\n",
    "\n",
    "        count_tags = len([x.name for x in soup.find_all()])\n",
    "        count_unique_tags = len(Counter([x.name for x in soup.find_all()]))\n",
    "\n",
    "        return [size, html_len, count_links, count_buttons,\n",
    "                count_inputs, count_ul, count_ol, count_lists,\n",
    "                count_h1, count_h2, total_h1_len, total_h2_len, avg_h1_len, avg_h2_len,\n",
    "                count_images, count_tags, count_unique_tags,\n",
    "                parser_data]\n",
    "    except:\n",
    "        return [-1, -1, -1, -1,\n",
    "                -1, -1, -1, -1,\n",
    "                -1, -1, -1, -1, -1, -1,\n",
    "                -1, -1, -1,\n",
    "                \"no data\"]\n",
    "\n",
    "\n",
    "clickbait_html = pickle.load(open('data/clickbait_html.pkl','rb'))\n",
    "clickbait_features = Parallel(n_jobs=30)(delayed(features)(html) for html in tqdm(clickbait_html))\n",
    "\n",
    "clickbait_features = pd.DataFrame(clickbait_features,\n",
    "                                     columns=[\"size\", \"html_len\", \"number_of_links\", \"number_of_buttons\",\n",
    "                                              \"number_of_inputs\", \"number_of_ul\", \"number_of_ol\", \"number_of_lists\",\n",
    "                                              \"number_of_h1\", \"number_of_h2\", \"total_h1_len\", \"total_h2_len\",\n",
    "                                              \"avg_h1_len\", \"avg_h2_len\",\n",
    "                                              \"number_of_images\", \"number_of_tags\", \"number_of_unique_tags\",\n",
    "                                              \"textdata\"])\n",
    "\n",
    "clickbait_features.to_csv('data/clickbait_website_features.csv', index=False, encoding='utf-8')\n",
    "\n",
    "non_clickbait_html = pickle.load(open('data/non_clickbait_html.pkl','rb'))\n",
    "non_clickbait_features = Parallel(n_jobs=30)(delayed(features)(html) for html in tqdm(non_clickbait_html))\n",
    "\n",
    "non_clickbait_features = pd.DataFrame(non_clickbait_features,\n",
    "                                         columns=[\"size\", \"html_len\", \"number_of_links\", \"number_of_buttons\",\n",
    "                                                  \"number_of_inputs\", \"number_of_ul\", \"number_of_ol\", \"number_of_lists\",\n",
    "                                                  \"number_of_h1\", \"number_of_h2\", \"total_h1_len\", \"total_h2_len\",\n",
    "                                                  \"avg_h1_len\", \"avg_h2_len\",\n",
    "                                                  \"number_of_images\", \"number_of_tags\", \"number_of_unique_tags\",\n",
    "                                                  \"textdata\"])\n",
    "\n",
    "\n",
    "non_clickbait_features.to_csv('data/non_clickbait_website_features.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YmMZH4zbGcMF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "02_dexters_scrapper.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
